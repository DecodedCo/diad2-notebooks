{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud - Jupyter Notebook\n",
    "\n",
    "## Welcome to your notebook!\n",
    "\n",
    "This is where you will read, write, and execute Python code. We will work through this notebook together, but you'll find notes including between code cells to help you keep on track.\n",
    "\n",
    "Developers typically start using a new programming language by figuring out how to get the computer to output \"Hello World.\" You can do so here by clicking into the code cell below and typing *shift-Enter* or by clicking the \"Run\" button above. Then, modify the code to print out another message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Our dataset, stored in **data/creditcard.csv**, is from a publicly available set of [credit card transactions](https://www.kaggle.com/mlg-ulb/creditcardfraud). These card present transactions are from European cardholders in September 2013. This dataset is commonly referenced in research literature in the fraud space.\n",
    "\n",
    "Our ability to make good predictions depends on the data we use -- what differences might you expect between the model we will make based on this dataset and models built on more recent data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/creditcard_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "\n",
    "We want to build a decision tree that is able to predict whether a certain transaction is fraudulent based on the data available to us.\n",
    "\n",
    "Again, we won't start from scratch; we'll use a data science toolkit called [sklearn](https://scikit-learn.org/stable/modules/tree.html), but we'll need to specify what data we are using as input and which column we want to predict as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Use all data except the 'Class' column as input\n",
    "X = data.drop('Class', axis=1)\n",
    "# Use the 'Class' column as what we want to predict as output\n",
    "y = data['Class']\n",
    "\n",
    "# Create an empty model \n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to our data\n",
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "We've built our tree -- now let's test it.\n",
    "\n",
    "In the cell below, use **model.score(X, y)** to evaluate the accuracy of our tree using our input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model iteration\n",
    "\n",
    "Just like much of writing is reading and re-writing, when data scientists test their models, they analyze the results and re-build the models.\n",
    "\n",
    "What might explain the accuracy score of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X and y (our input and outputs) into training and testing datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code we used to build the model before; **modify it to use your training and testing datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X, y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what might explain the accuracy score of your model?\n",
    "\n",
    "With data about a transaction and no model to form a prediction, what would you guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.value_counts(data[\"Class\"])\n",
    "\n",
    "print(counts)\n",
    "counts.plot(kind=\"bar\",\n",
    "           title=\"Frequency of Genuine vs Fraudulent Transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "Data scientists make choices that impact model outputs. To deal with this class imbalance, we could choose to oversample the minority class or undersample the majority; there are trade offs with each.\n",
    "\n",
    "With more time, we might test multiple strategies. Today we'll undersample the number of genuine transactions.\n",
    "\n",
    "**Modify the code below to sample the number of genuine transactions that balance the classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many genuine transactions should we use to balance the classes?\n",
    "number_genuine = 1\n",
    "\n",
    "# Separate genuine transactions and fraud\n",
    "genuine = data[data[\"Class\"] == 0].sample(number_genuine)\n",
    "fraud = data[data[\"Class\"] == 1]\n",
    "\n",
    "# Combine fraud and genuine\n",
    "even_data = pd.concat([genuine, fraud])\n",
    "\n",
    "# Summarize our new dataset, even_data\n",
    "even_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we used the correlation matrix to get a sense of the predictive power of our intial dataset, \"data\".\n",
    "\n",
    "**Modify the code below to view the correlation matrix of our new dataset, \"even_data\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a new dataset, we'll need to recreate our inputs, outputs, and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs and outputs with new dataset\n",
    "X = even_data.drop('Class', axis=1)\n",
    "y = even_data['Class']\n",
    "\n",
    "# Split new inputs and outputs into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "# Train and score decision tree using new data\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll notice we're repeating a lot of the same code -- let's put it in a function to make it easier to use later.\n",
    "\n",
    "Run the cell below to *define* a function called fit_and_score_model, which creates a decision tree model to predict the 'Class' column using the dataset you specify. When you provide information to a function, put it in the parentheses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score_model(data, max_depth=None):\n",
    "    # Create inputs and outputs\n",
    "    X = data.drop('Class', axis=1)\n",
    "    y = data['Class']\n",
    "    \n",
    "    # Split inputs and outputs into training and testing\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "    # Train and score decision tree\n",
    "    model = tree.DecisionTreeClassifier(max_depth=None)\n",
    "    model = model.fit(X_train, y_train)\n",
    "    print(model.score(X_test, y_test))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use the function we just defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model = fit_and_score_model(even_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independent model building\n",
    "\n",
    "We can continue to iterate on our models with a few additional tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_columns(data, column_names):\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    rob_scaler = RobustScaler()\n",
    "    temp_data = data.copy()\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        if column_name == \"\": print(\"Enter a column name or list of names that you'd like scaled!\"); return;\n",
    "        \n",
    "        temp_data[column_name] = rob_scaler.fit_transform(temp_data[column_name].values.reshape(-1,1))\n",
    "    return temp_data\n",
    "\n",
    "def drop_outliers(data, column_names, fraud=1, threshold=1.5):\n",
    "    import numpy as np\n",
    "    \n",
    "    for column_name in column_names:\n",
    "        fraud_values = data[column_name][data[\"Class\"] == fraud].values\n",
    "        q25, q75 = np.percentile(fraud_values, 25), np.percentile(fraud_values, 75)\n",
    "        iqr = q75 - q25\n",
    "        lower, upper = q25 - (iqr * threshold), q75 + (iqr * threshold)\n",
    "        data = data.drop(data[(data[column_name] > upper) | (data[column_name] < lower)].index)\n",
    "    return data\n",
    "\n",
    "print(\"Functions successfully loaded: \")\n",
    "print(\"*\\t my_data = scale_columns(data, [\\\"Column\\\", \\\"Name(s)\\\"])\")\n",
    "print(\"*\\t my_data = drop_outliers(data, [\\\"Column\\\", \\\"Name(s)\\\"], fraud=1, threshold=1.5)\")\n",
    "print(\"*\\t my_model = fit_and_score_model(data, max_depth=5)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make a fresh copy of our balanced dataset called my_data so we can experiment.  If you ever want to go back to the balanced dataset, run the cell below again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_data = even_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, using the options above, create your own dataset, use it to build a new model, and test it to see how accurate you're able to make it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
