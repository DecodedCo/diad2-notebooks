{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud - Jupyter Notebook\n",
    "\n",
    "## Welcome to your notebook!\n",
    "\n",
    "This is where you will read, write, and execute Python code. We will work through this notebook together, but you'll find notes including between code cells to help you keep on track.\n",
    "\n",
    "Developers typically start using a new programming language by figuring out how to get the computer to output \"Hello World.\" You can do so here by clicking into the code cell below and typing *shift-Enter* or by clicking the \"Run\" button above. Then, modify the code to print out another message!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the dataset\n",
    "\n",
    "Our dataset, stored in **data/creditcard.csv**, is from a publicly available set of [credit card transactions](https://www.kaggle.com/mlg-ulb/creditcardfraud). These card present transactions are from European cardholders in September 2013. This dataset is commonly referenced in research literature in the fraud space.\n",
    "\n",
    "Our ability to make good predictions depends on the data we use -- what differences might you expect between the model we will make based on this dataset and models built on more recent data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/creditcard_data.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice how many column headers are anonymized - this mimics what Mastercard sees in Decision Intelligence. Since our model will learn using the data, it doesn't need to know what each number represents.**\n",
    "\n",
    "Why do you think it is useful to have anonymzied column names and scaled values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building our model\n",
    "\n",
    "**Mastercard uses a variety of types models to detect fraud. As you know, one of them is decision trees.**\n",
    "\n",
    "Today, we will build a decision tree that is able to predict whether a certain transaction is fraudulent based on the data available to us.\n",
    "\n",
    "Again, we won't start from scratch; we'll use a data science toolkit called [sklearn](https://scikit-learn.org/stable/modules/tree.html), but we'll need to specify what data we are using as input and which column we want to predict as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "# Use all data except the 'Class' column as input\n",
    "X = data.drop('Class', axis=1)\n",
    "# Use the 'Class' column as what we want to predict as output\n",
    "y = data['Class']\n",
    "\n",
    "# Create an empty model \n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to our data\n",
    "model = model.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our model\n",
    "\n",
    "We've built our tree -- now let's test it.\n",
    "\n",
    "In the cell below, use **model.score(X, y)** to evaluate the accuracy of our tree using our input and output data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model iteration\n",
    "\n",
    "Just like much of writing is reading and re-writing, when data scientists test their models, they analyze the results and re-build the models.\n",
    "\n",
    "What might explain the accuracy score of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X and y (our input and outputs) into training and testing datasets.\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code we used to build the model before; **modify it to use your training and testing datasets.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X, y)\n",
    "model.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what might explain the accuracy score of your model?\n",
    "\n",
    "With data about a transaction and no model to form a prediction, what would you guess?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.value_counts(data[\"Class\"])\n",
    "\n",
    "print(counts)\n",
    "counts.plot(kind=\"bar\",\n",
    "           title=\"Frequency of Genuine vs Fraudulent Transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this a problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "**Data scientists make choices that impact model outputs. At Mastercard, data scientists are dealing with the same challenge: trying to reduce fraud based on limited datasets.**\n",
    "\n",
    "To deal with the uneven number of fraud and genuine transactions, we could artificially increase the number of fraud transactions by creating similar transactions, or we could reduce the number of genuine transactions.\n",
    "\n",
    "With more time, we might test multiple strategies. Today we'll reduce the number of genuine transactions.\n",
    "\n",
    "**Modify the code below to sample the number of genuine transactions that balance the classes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many genuine transactions should we use to balance the classes?\n",
    "number_genuine = 1\n",
    "\n",
    "# Separate genuine transactions and fraud\n",
    "genuine = data[data[\"Class\"] == 0].sample(number_genuine)\n",
    "fraud = data[data[\"Class\"] == 1]\n",
    "\n",
    "# Combine fraud and genuine\n",
    "even_data = pd.concat([genuine, fraud])\n",
    "\n",
    "# Summarize our new dataset, even_data\n",
    "even_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Previously, we used the correlation matrix to get a sense of the predictive power of our intial dataset, \"data\".\n",
    "\n",
    "**Modify the code below to view the correlation matrix of our new dataset, \"even_data\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "correlation_matrix = data.corr()\n",
    "sns.heatmap(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a new dataset, we'll need to recreate our inputs, outputs, and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs and outputs with new dataset\n",
    "X = even_data.drop('Class', axis=1)\n",
    "y = even_data['Class']\n",
    "\n",
    "# Split new inputs and outputs into training and testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=99)\n",
    "\n",
    "# Train and score decision tree using new data\n",
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
