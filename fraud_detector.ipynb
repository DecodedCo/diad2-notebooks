{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Credit Card Fraud - Jupyter Notebook\n",
    "<br><br>\n",
    "<b>Notebook objective:</b> build a machine learning fraud detection engine in Python\n",
    "\n",
    "<h2 style=\"background-color:DarkCyan; text-align:center\"><br>Step 1: Python intro<br></h2>\n",
    "<br>\n",
    "To introduce you to coding in Python, you're going to run code that prints \"Hello World!\"\n",
    "<br><br>\n",
    "<mark>Click in the gray cell below and hit Shift + Enter to run the code. If it works, you will see text printed out beneath the cell. Edit the code and re-run it to make it print out your name!</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hello World!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"background-color:Tomato; text-align:center\"><br>Step 2: Loading the dataset<br></h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to read in our csv file of data. To do this with raw Python, we'd have to write a lot of code. Fortunately, someone ([Wes Mckinney](https://wesmckinney.com/pages/about.html)) created a \"library\" of Python code that packages up all that code in to a simple function. The library is called Pandas (Python ANd Data Science), which we can import and give a short nickname (\"pd\").\n",
    "<br><br>\n",
    "<mark>Run the code below, without editing it, the same way you did above. If everything works you will get a number next to the cell, and no error.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"data/creditcard_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Run the code below to see the top (\"head\") 5 rows of the data. Scroll left and right to see all the columns. Then change the number and re-run the code to see what it does.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>Run the code below to see summary statistics for the entire dataset. Can you find the average amount of the transactions in this dataset?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2 style=\"background-color:DodgerBlue; text-align:center\"><br>Step 3: Building our model<br></h2>\n",
    "<br>\n",
    "<b>Mastercard uses a variety of types models as part of Decision Intelligence to detect fraud. One of them is decision trees.</b>\n",
    "\n",
    "<mark>Read the code below and try to understand what it is doing. The greenish gray text after \"#\" are comments - little bits of text to explain the code, they don't do anything other than explain the code. Once you're happy, run the code and hope for no errors!</mark> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the most popular library of code for making decision trees (found by googling)\n",
    "from sklearn import tree\n",
    "\n",
    "# Use all data except the 'Fraud' column as input\n",
    "X = data.drop('Fraud', axis=1)\n",
    "\n",
    "# Use the 'Fraud' column as what we want to predict as output\n",
    "Y = data['Fraud']\n",
    "\n",
    "# Create an empty model \n",
    "model = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Fit the model to our data\n",
    "model = model.fit(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've built our tree -- now let's test it.\n",
    "\n",
    "<mark>Run the code below to evaluate the accuracy of our tree using our input and output data.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score(X,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy ranges between 0.0 (it predicted every transaction wrong) to 1.0 (it predicted every transaction right).\n",
    "\n",
    "<mark>Look at your accuracy and consider: is it possible to be too accurate?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2 style=\"background-color:MediumSeaGreen; text-align:center\"><br>Step 4: Evaluating our model<br></h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model iteration\n",
    "\n",
    "Just like much of writing is reading and re-writing, when data scientists build their models, they constantly test and re-build them.\n",
    "\n",
    "<mark>Run the code below to split the data into training X and Y and test X and Y (this creates four sets of data)</mark>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split X and y (our input and outputs) into training and testing datasets.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code we used to build the model before.\n",
    "\n",
    "<mark><b>Modify the code</b> so that it fits on your training data and scores on your testing data. (Hint: look at the code in the cell above)</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tree.DecisionTreeClassifier()\n",
    "model = model.fit(X, Y)\n",
    "model.score(X, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, what might explain the accuracy score of your model?\n",
    "\n",
    "<mark>Run the code below and interpret the results. What problem is this showing?</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = pd.value_counts(data['Fraud'])\n",
    "\n",
    "%matplotlib inline\n",
    "print(counts)\n",
    "counts.plot(kind=\"bar\",\n",
    "           title=\"Frequency of Genuine vs Fraudulent Transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this a problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2 style=\"background-color:DarkOrange; text-align:center\"><br>Step 5: Improving our model<br></h2>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transforming the data\n",
    "\n",
    "**Data scientists make choices that impact model outputs. At Mastercard, data scientists are dealing with the same challenge: trying to reduce fraud based on limited datasets.**\n",
    "\n",
    "To deal with the uneven number of fraud and genuine transactions, we could artificially increase the number of fraud transactions by creating similar transactions, or we could reduce the number of genuine transactions.\n",
    "\n",
    "With more time, we might test multiple strategies. Today we'll just try reducing the number of genuine transactions.\n",
    "\n",
    "<mark>Run the code, and look at the mean of the Class column. What does it mean? <b>Modify number_genuine to change the number of genuine transactions that balance the classes, and re-run the code</b>.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many genuine transactions should we use to balance the classes?\n",
    "number_genuine = 1\n",
    "\n",
    "# Separate genuine transactions and fraud\n",
    "genuine = data[data['Fraud'] == 0].sample(number_genuine)\n",
    "fraud = data[data['Fraud'] == 1]\n",
    "\n",
    "# Combine fraud and genuine\n",
    "even_data = pd.concat([genuine, fraud])\n",
    "\n",
    "# Summarize our new dataset, even_data\n",
    "even_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a new dataset, we'll need to recreate our inputs, outputs, and split them into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inputs and outputs with new dataset\n",
    "X = even_data.drop('Fraud', axis=1)\n",
    "Y = even_data['Fraud']\n",
    "\n",
    "# Split new inputs and outputs into training and testing\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=99)\n",
    "\n",
    "# Train and score decision tree using new data\n",
    "model = tree.DecisionTreeClassifier(max_depth = 1)\n",
    "model = model.fit(X_train, Y_train)\n",
    "model.score(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to visualize the decision tree we made. To do this, we've copied some code from the sklearn documentation.\n",
    "\n",
    "<mark>Run the code below to see your tree!</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphviz\n",
    "dot_data = tree.export_graphviz(model, out_file=None, \n",
    "                     feature_names=X.columns.values,  \n",
    "                     class_names=[\"Genuine\",\"Fraud\"],  \n",
    "                     filled=True, rounded=True, \n",
    "                     special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might notice your decision tree is very small. The final step is to evaluate how complex your decision tree needs to be.\n",
    "\n",
    "<mark>Go back <b>two</b> code cells and change the max_depth of your decsion tree (line 9). Run the code, then re-run the code to visualize the tree</mark>\n",
    "\n",
    "<mark>What size of decision tree gets the greatest accuracy for your data? Why?</mark>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<h2 style=\"background-color:Gold; text-align:center\"><br>Step 6: Quantifying the investment<br></h2>\n",
    "<br>\n",
    "\n",
    "**Building good models requires time and resources; it is important to focus on valuable investments.**\n",
    "\n",
    "How do you know your time was well spent?\n",
    "\n",
    "There are huge costs associated with accepting a fraudlent transaction; [LexisNexis](https://risk.lexisnexis.com/insights-resources/research/2018-true-cost-of-fraud-study-for-the-retail-sector) finds fraud costs retailers an average of $2.94 per fraudulent dollar in fees, prevention, legal costs, etc. Declining a genuine transaction is costly too! [Ayden and 451 Research](https://go.adyen.com/rs/222-DNK-376/images/Retail%20Report%202019.pdf?mkt_tok=eyJpIjoiWXpNeE56Y3paRGszTnpBNSIsInQiOiJaVmJ1NXVJVkZFMkdHY1FCYVRGUENFemlDWnU3RSthM21LRmF3MDdtUldwSjZvMVF6ZzVjTTFjemJKS1BxZUJWWElxejZrQXVKeDhwNlZGVXkwT3FtcTkwd1BFTkwwaWZlV1BFcnM3YmY2aEQ1RnMrT3BFS1g4MTRsaWI3R1BUSSJ9) report 2 in 5 consumers have abandoned a purchase after a declined payment in the past 6 months. Customers are less likely to return to a merchant after a failed payment.\n",
    "\n",
    "In our simulation, we'll charge $2.94 per dollar for each false approval and the cost of the transaction for each false decline. Of course, every situation is different, and a client will likely have their own costs associated with false approvals and declines.\n",
    "\n",
    "<mark>Run the cell below to compare the cost of fraud when using your model with the cost of approving all transactions.</mark>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COST_PER_FRAUD_DOLLAR = 2.94 # Cost per dollar of a false approval\n",
    "COST_PER_FALSE_DECLINE_DOLLAR = 1 # Cost per dollar of a false decline\n",
    "\n",
    "predictions = list(model.predict(X_test))\n",
    "truth = list(Y_test)\n",
    "\n",
    "false_approval_cost = 0\n",
    "false_approval_num = 0\n",
    "false_decline_cost = 0\n",
    "false_decline_num = 0\n",
    "correct_num = 0\n",
    "correct_cost = 0\n",
    "\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] != truth[i]: # If our prediction was wrong\n",
    "        if truth[i] == 1: # If we falsely approved\n",
    "            false_approval_cost += (X_test.iloc[i, 0] * COST_PER_FRAUD_DOLLAR) # Cost increases by $2.94 * the amount of the transaction\n",
    "            false_approval_num += 1\n",
    "        else: # If we falsely decline\n",
    "            false_decline_cost += (X_test.iloc[i, 0] * COST_PER_FALSE_DECLINE_DOLLAR) # We miss a sale, cost increases by the amount of the transaction\n",
    "            false_decline_num += 1\n",
    "    else: # If our prediction was correct\n",
    "        correct_num += 1\n",
    "        if truth[i] == 0: # It's a genuine transaction\n",
    "            correct_cost += X_test.iloc[i, 0]\n",
    "print(\"You processed {} payments. {} were correct predictions, of which the genuine transactions totalled ${} in revenue.\\n\\nYou had {} false approvals, which cost ${} in fees and administrative costs.\\nYou had {} false declines, which cost ${} in missed sales.\\n\".format(len(predictions), correct_num, round(correct_cost,2),  false_approval_num, round(false_approval_cost, 2), false_decline_num, round(false_decline_cost,2)))\n",
    "print(\"{}% of your predictions were incorrect.\\nYour loss due to fraud was {}% of revenue.\\n\".format(round((false_approval_num+false_decline_num)*100/len(predictions),2), round((false_approval_cost+false_decline_cost)/(false_approval_cost+false_decline_cost+correct_cost)*100,2)))\n",
    "\n",
    "approve_all_cost = 0\n",
    "approve_all_num = 0\n",
    "all_genuine_cost = 0\n",
    "\n",
    "for i in range(len(truth)):\n",
    "    if truth[i] == 1: # There was fraud\n",
    "        approve_all_cost += (X_test.iloc[i, 0] * COST_PER_FRAUD_DOLLAR)\n",
    "        approve_all_num += 1\n",
    "    else: # Genuine transaction\n",
    "        all_genuine_cost += X_test.iloc[i, 0]\n",
    "\n",
    "print(\"If you had simply approved all {} transactions, you would have falsely approved {} transactions, costing ${} while earning ${} in revenue.\\n\".format(len(predictions), approve_all_num, round(approve_all_cost, 2), round(all_genuine_cost, 2)))\n",
    "print(\"Your model's predictions were worth ${}.\".format(round(((correct_cost - false_decline_cost - false_approval_cost)-(all_genuine_cost - approve_all_cost)),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<mark>What differs in our simulation when compared to reality?</mark>\n",
    "\n",
    "<mark>What could make our model stronger?</mark>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
